{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.optim as optim\n",
    "\n",
    "import h5py\n",
    "import scipy.io\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 0.06\n",
    "e = 0.045\n",
    "l_1 = 0.176\n",
    "g = 9.8\n",
    "alpha = torch.tensor([-30, 90, 210], dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_only_Friction(nn.Module):\n",
    "    \n",
    "    #----------------------------Initialize the Network--------------------------#\n",
    "    def __init__(self, input_dim = 10, output_dim = 6, hidden_layer_dim = 64, num_hidden_layer = 3, activation = 'tanh'):\n",
    "        super(PINN_only_Friction, self).__init__()\n",
    "        \n",
    "        #----------------------Initializing Parameters----------------------------#\n",
    "        self.l_1c = 85 * (10 ** -3)\n",
    "        self.m_0 = 14 * (10 ** -3)\n",
    "        self.m_1 = 22.80 * (10 ** -3)\n",
    "        self.m_2 = 150 * (10 ** -3)\n",
    "        self.I_1 = 256400.64 * (10 ** -9) + self.m_1 * l_1 ** 2\n",
    "\n",
    "        self.layers = nn.ModuleList([nn.Linear(input_dim, hidden_layer_dim)])\n",
    "        for _ in range(num_hidden_layer - 1):\n",
    "            self.layers.append(nn.Linear(hidden_layer_dim, hidden_layer_dim))\n",
    "        self.layers.append(nn.Linear(hidden_layer_dim, output_dim))\n",
    "\n",
    "        self.epoch = 0\n",
    "        \n",
    "        \n",
    "        #---------------------Differences compare to Comprehensive model-------------------#\n",
    "        self.f_c1 = nn.Parameter(torch.tensor(np.random.rand()), requires_grad = True)\n",
    "        self.f_c2 = nn.Parameter(torch.tensor(np.random.rand()), requires_grad = True)\n",
    "        self.f_c3 = nn.Parameter(torch.tensor(np.random.rand()), requires_grad = True)\n",
    "\n",
    "        self.f_v1 = nn.Parameter(torch.tensor(np.random.rand()), requires_grad = True)\n",
    "        self.f_v2 = nn.Parameter(torch.tensor(np.random.rand()), requires_grad = True)\n",
    "        self.f_v3 = nn.Parameter(torch.tensor(np.random.rand()), requires_grad = True)\n",
    "\n",
    "\n",
    "        if activation == 'sin':\n",
    "          self.activation = torch.sin\n",
    "        elif activation == 'tanh':\n",
    "          self.activation = torch.tanh\n",
    "        elif activation == 'relu':\n",
    "          self.activation = torch.nn.ReLU()\n",
    "        else:\n",
    "          self.activation = torch.nn.LeakyReLU()\n",
    "          \n",
    "    #------------------------Forward Pass---------------------------#      \n",
    "    def forward(self, t, tau, s, s_Ddot):\n",
    "        out = torch.cat([t, tau, s, s_Ddot], dim=-1)\n",
    "        for layer in self.layers[:-1]:\n",
    "            out = self.activation(layer(out))\n",
    "        out = self.layers[-1](out)\n",
    "        return out\n",
    "      \n",
    "    #------------------------Define Inverse Dynamic equations-----------------#\n",
    "    def Inverse_Dynamic_Eq(self, t, tau, s, s_Ddot):\n",
    "        f_c1 = torch.nn.functional.softplus(self.f_c1)\n",
    "        f_c2 = torch.nn.functional.softplus(self.f_c2)\n",
    "        f_c3 = torch.nn.functional.softplus(self.f_c3)\n",
    "        f_v1 = torch.nn.functional.softplus(self.f_v1)\n",
    "        f_v2 = torch.nn.functional.softplus(self.f_v2)\n",
    "        f_v3 = torch.nn.functional.softplus(self.f_v3)\n",
    "      \n",
    "        t = t.requires_grad_()\n",
    "        u_pred = self.forward(t, tau, s, s_Ddot)   # batch_size x 6\n",
    "\n",
    "        theta, theta_dot = torch.chunk(u_pred, 2, dim = 1) # each has dimension of: batch_size x 3\n",
    "\n",
    "        theta_1 = theta[:, 0:1]                     # shape: batch_size x 1\n",
    "        theta_2 = theta[:, 1:2]                     # shape: batch_size x 1\n",
    "        theta_3 = theta[:, 2:3]                     # shape: batch_size x 1\n",
    "\n",
    "        theta_1_dot = theta_dot[:, 0:1]             # shape: batch_size x 1\n",
    "        theta_2_dot = theta_dot[:, 1:2]             # shape: batch_size x 1\n",
    "        theta_3_dot = theta_dot[:, 2:3]             # shape: batch_size x 1\n",
    "\n",
    "        \n",
    "        #---------------Define M matrix------------------------#\n",
    "        M = torch.cat(                                                                                                  # 6 x 6\n",
    "                    [torch.cat([torch.eye(3), torch.zeros(3, 3)], dim=1),                                               # 3 x 6\n",
    "                     torch.cat([torch.zeros(3, 3), (2 * self.I_1 + self.m_2 * (l_1 ** 2)) * torch.eye(3)], dim=1)],     # 3 x 6\n",
    "                     dim=0\n",
    "                     ).unsqueeze(0).expand(u_pred.shape[0], -1, -1)\n",
    "        \n",
    "        #M = M.to(device)\n",
    "\n",
    "\n",
    "        #---------------Define G matrix------------------------#\n",
    "        G_1 = (self.m_1 * self.l_1c + self.m_2 * l_1) * g * torch.cos(torch.deg2rad(theta_1))               # batch_size x 1\n",
    "        G_2 = (self.m_1 * self.l_1c + self.m_2 * l_1) * g * torch.cos(torch.deg2rad(theta_2))               # batch_size x 1\n",
    "        G_3 = (self.m_1 * self.l_1c + self.m_2 * l_1) * g * torch.cos(torch.deg2rad(theta_3))               # batch_size x 1\n",
    "\n",
    "        G = torch.cat([G_1, G_2, G_3], dim = 1)                                                             # batch_size x 3\n",
    "        #G = G.to(device)\n",
    "\n",
    "        #----------------Define Friction matrices---------------#\n",
    "        F_v = torch.zeros(3, 3)\n",
    "        F_v[0, 0] = self.f_v1\n",
    "        F_v[1, 1] = self.f_v2\n",
    "        F_v[2, 2] = self.f_v3\n",
    "        F_v = F_v.unsqueeze(0).expand(s.shape[0],-1, -1)\n",
    "        \n",
    "        F_c = torch.zeros(3, 3)\n",
    "        F_c[0, 0] = self.f_c1\n",
    "        F_c[1, 1] = self.f_c2\n",
    "        F_c[2, 2] = self.f_c3\n",
    "        F_c = F_c.unsqueeze(0).expand(s.shape[0],-1, -1)\n",
    "        \n",
    "        #F_v = F_v.to(device)\n",
    "        #F_c = F_c.to(device)\n",
    "\n",
    "        #---------------Computing derivatives----------------#\n",
    "        theta_1_dot_t = torch.autograd.grad(theta_1, t, grad_outputs = torch.ones_like(theta_1), create_graph=True)[0]\n",
    "        theta_2_dot_t = torch.autograd.grad(theta_2, t, grad_outputs = torch.ones_like(theta_2), create_graph=True)[0]\n",
    "        theta_3_dot_t = torch.autograd.grad(theta_3, t, grad_outputs = torch.ones_like(theta_3), create_graph=True)[0]\n",
    "\n",
    "        theta_1_Ddot_t = torch.autograd.grad(theta_1_dot, t, grad_outputs = torch.ones_like(theta_1_dot), create_graph=True)[0]\n",
    "        theta_2_Ddot_t = torch.autograd.grad(theta_2_dot, t, grad_outputs = torch.ones_like(theta_2_dot), create_graph=True)[0]\n",
    "        theta_3_Ddot_t = torch.autograd.grad(theta_3_dot, t, grad_outputs = torch.ones_like(theta_3_dot), create_graph=True)[0]\n",
    "\n",
    "\n",
    "        #----------------Define du_t = [theta_1_dot theta_2_dot theta_3_dot theta_1_Ddot theta_2_Ddot theta_3_Ddot]--------------#\n",
    "        du_t = torch.cat([theta_1_dot_t, theta_2_dot_t, theta_3_dot_t, theta_1_Ddot_t, theta_2_Ddot_t, theta_3_Ddot_t], dim = 1).unsqueeze(-1)   # batch_size x 6 x 1\n",
    "\n",
    "        #du_t = du_t.to(device)\n",
    "        \n",
    "        #----------------Define K matrix---------------------#\n",
    "        K11 = (s[:, 0] * torch.cos(torch.deg2rad(alpha[0])) + s[:, 1] * torch.sin(torch.deg2rad(alpha[0])) + f - e) * torch.sin(torch.deg2rad(theta[:, 0])) - s[:, 2] * torch.cos(torch.deg2rad(theta[:, 0]))\n",
    "        K22 = (s[:, 0] * torch.cos(torch.deg2rad(alpha[1])) + s[:, 1] * torch.sin(torch.deg2rad(alpha[1])) + f - e) * torch.sin(torch.deg2rad(theta[:, 1])) - s[:, 2] * torch.cos(torch.deg2rad(theta[:, 1]))\n",
    "        K33 = (s[:, 0] * torch.cos(torch.deg2rad(alpha[2])) + s[:, 1] * torch.sin(torch.deg2rad(alpha[2])) + f - e) * torch.sin(torch.deg2rad(theta[:, 2])) - s[:, 2] * torch.cos(torch.deg2rad(theta[:, 2]))\n",
    "\n",
    "        K = torch.zeros((s.shape[0], 3, 3))    # batch_size x 3 x 3\n",
    "        K[:, 0, 0] = K11\n",
    "        K[:, 1, 1] = K22\n",
    "        K[:, 2, 2] = K33\n",
    "\n",
    "        #----------------------------------------------------#\n",
    "        a_11 = s[:, 0] + e * torch.cos(torch.deg2rad(alpha[0])) - f * torch.cos(torch.deg2rad(alpha[0])) - l_1 * torch.cos(torch.deg2rad(alpha[0])) * torch.cos(torch.deg2rad(theta[:, 0])) # shape: batch_size\n",
    "        a_12 = s[:, 0] + e * torch.cos(torch.deg2rad(alpha[1])) - f * torch.cos(torch.deg2rad(alpha[1])) - l_1 * torch.cos(torch.deg2rad(alpha[1])) * torch.cos(torch.deg2rad(theta[:, 1])) # shape: batch_size\n",
    "        a_13 = s[:, 0] + e * torch.cos(torch.deg2rad(alpha[2])) - f * torch.cos(torch.deg2rad(alpha[2])) - l_1 * torch.cos(torch.deg2rad(alpha[2])) * torch.cos(torch.deg2rad(theta[:, 2])) # shape: batch_size\n",
    "\n",
    "        a_21 = s[:, 1] + e * torch.sin(torch.deg2rad(alpha[0])) - f * torch.sin(torch.deg2rad(alpha[0])) - l_1 * torch.sin(torch.deg2rad(alpha[0])) * torch.cos(torch.deg2rad(theta[:, 0])) # shape: batch_size\n",
    "        a_22 = s[:, 1] + e * torch.sin(torch.deg2rad(alpha[1])) - f * torch.sin(torch.deg2rad(alpha[1])) - l_1 * torch.sin(torch.deg2rad(alpha[1])) * torch.cos(torch.deg2rad(theta[:, 1])) # shape: batch_size\n",
    "        a_23 = s[:, 1] + e * torch.sin(torch.deg2rad(alpha[2])) - f * torch.sin(torch.deg2rad(alpha[2])) - l_1 * torch.sin(torch.deg2rad(alpha[2])) * torch.cos(torch.deg2rad(theta[:, 2])) # shape: batch_size\n",
    "\n",
    "        a_31 = s[:, 2] - l_1 * torch.cos(torch.deg2rad(theta[:, 0])) # shape: batch_size\n",
    "        a_32 = s[:, 2] - l_1 * torch.cos(torch.deg2rad(theta[:, 1])) # shape: batch_size\n",
    "        a_33 = s[:, 2] - l_1 * torch.cos(torch.deg2rad(theta[:, 2])) # shape: batch_size\n",
    "\n",
    "        #-------------Define A matrix--------#\n",
    "        A = torch.zeros((s.shape[0], 3, 3))\n",
    "\n",
    "        A[:, 0, 0] = a_11\n",
    "        A[:, 0, 1] = a_12\n",
    "        A[:, 0, 2] = a_13\n",
    "        A[:, 1, 0] = a_21\n",
    "        A[:, 1, 1] = a_22\n",
    "        A[:, 1, 2] = a_23\n",
    "        A[:, 2, 0] = a_31\n",
    "        A[:, 2, 1] = a_32\n",
    "        A[:, 2, 2] = a_33\n",
    "        \n",
    "        #------------Define B matrix-----------#\n",
    "        B = torch.zeros((s.shape[0], 3, 1))\n",
    "        \n",
    "        B[:, 0, 0] = (self.m_0 + 3 * self.m_2) * s_Ddot[:, 0]\n",
    "        B[:, 1, 0] = (self.m_0 + 3 * self.m_2) * s_Ddot[:, 1]\n",
    "        B[:, 2, 0] = (self.m_0 + 3 * self.m_2) * (s_Ddot[:, 2] - g)\n",
    "\n",
    "        #equation = ((M @ du_t) -                                                                                                                                                # [batch_size x 6 x 6 @ batch_size x 6 x 1] = batch_size x 6 x 1\n",
    "                    #(torch.cat((theta_dot.unsqueeze(-1), - G.unsqueeze(-1) + K @ torch.linalg.inv(A) @ B - (F_v @ theta_dot.unsqueeze(-1)) - ((F_c @ torch.sign(theta_dot.unsqueeze(-1))))), dim = 1)) -    # batch_size x 3 -> unsqueeze(-1) = batch_size x 3 x 1\n",
    "                                                                                                                                                                                                  # batch_size x 3 x 3 @ batch_size x 3 x 3 @ batch_size x 3 x 1 - [3 x 3 @ (batch_size x 3)T]T - [3 x 3 @ (batch_size x 3)T)]T = batch_size x 3 x 1\n",
    "                                                                                                                                                                                                  #                                                                       batch_size x 3 -> unsqueeze(-1) = batch_size x 3 x 1\n",
    "                                                                                                                                                                                                  # Attention: dim = 1 to concatenate along rows (1st dimension)\n",
    "                    #torch.cat([torch.zeros_like(tau), tau], dim = 1).unsqueeze(-1))     #   batch_size x 3\n",
    "                                                                                        #   batch_size x 3\n",
    "                                                                                        # = batch_size x 6 -> unsqueeze(-1) = batch_size x 6 x 1\n",
    "\n",
    "        equation = torch.cat((theta_dot.unsqueeze(-1), - G.unsqueeze(-1) + K @ torch.linalg.inv(A) @ B - (F_v @ theta_dot.unsqueeze(-1)) - ((F_c @ torch.sign(theta_dot.unsqueeze(-1))))), dim = 1)\n",
    "        equation = equation.squeeze(-1)\n",
    "\n",
    "        return equation\n",
    "    \n",
    "    #----------------------------------PDE Residual Loss function---------------------------------------#\n",
    "    def pde_loss(self, t, tau, s, s_Ddot):      \n",
    "        \n",
    "        loss_r1 = torch.mean(self.Inverse_Dynamic_Eq(t, tau, s, s_Ddot)[:, 3:4] ** 2)\n",
    "        loss_r2 = torch.mean(self.Inverse_Dynamic_Eq(t, tau, s, s_Ddot)[:, 4:5] ** 2) \n",
    "        loss_r3 = torch.mean(self.Inverse_Dynamic_Eq(t, tau, s, s_Ddot)[:, 5:6] ** 2) \n",
    "        return loss_r1, loss_r2, loss_r3\n",
    "    \n",
    "    #----------------------------------Data Loss function-----------------------------------------------#\n",
    "    def data_loss(self, t, tau, s, s_Ddot, u_data):   \n",
    "                                                                                                        # The first dimension across all argument is batch_size\n",
    "        pred = self.forward(t, tau, s, s_Ddot)                                                          # In order to compute the mean of pass-in values, we do not need to preserve the dimensions of tensor\n",
    "        loss_theta_1 = torch.mean((pred[:, 0:1] - u_data[:, 0:1]) ** 2)                                 # So that, we use [:, i] not [:, i:i+1]\n",
    "        loss_theta_2 = torch.mean((pred[:, 1:2] - u_data[:, 1:2]) ** 2) \n",
    "        loss_theta_3 = torch.mean((pred[:, 2:3] - u_data[:, 2:3]) ** 2)\n",
    "        loss_theta_1_dot = torch.mean((pred[:, 3:4] - u_data[:, 3:4]) ** 2) \n",
    "        loss_theta_2_dot = torch.mean((pred[:, 4:5] - u_data[:, 4:5]) ** 2) \n",
    "        loss_theta_3_dot = torch.mean((pred[:, 5:6] - u_data[:, 5:6]) ** 2) \n",
    "        return loss_theta_1, loss_theta_2, loss_theta_3, loss_theta_1_dot, loss_theta_2_dot, loss_theta_3_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------Adaptive Weight function-----------------------------------------#\n",
    "#---------------------------------Execute in each epoch---------------------------------------------#\n",
    "def adaptive_weight(model, X_train):\n",
    "    model.zero_grad()\n",
    "    param_tensors = [param for param in model.parameters()]\n",
    "    param_tensors_cpu = [param.cpu() for param in model.parameters()]\n",
    "\n",
    "    t = X_train['PDE']['t'].requires_grad_()     # total_data x 1\n",
    "    tau = X_train['PDE']['tau']                  # total_data x 3\n",
    "    s = X_train['PDE']['s']                      # total_data x 3\n",
    "    s_Ddot = X_train['PDE']['s_Ddot']            # total_data x 3\n",
    "\n",
    "\n",
    "    batch_size = 64\n",
    "    total_data = t.shape[0]\n",
    "    batches = (total_data + batch_size - 1) // batch_size\n",
    "\n",
    "    jacobian_r1 = torch.zeros(total_data, sum(p.numel() for p in param_tensors_cpu))\n",
    "    jacobian_r2 = torch.zeros(total_data, sum(p.numel() for p in param_tensors_cpu))\n",
    "    jacobian_r3 = torch.zeros(total_data, sum(p.numel() for p in param_tensors_cpu))\n",
    "\n",
    "    for batch in range(batches):\n",
    "      start_idx = batch * batch_size\n",
    "      end_idx = min(start_idx + batch_size, total_data)\n",
    "\n",
    "      t_batch = t[start_idx:end_idx]              # batch_size x 1\n",
    "      tau_batch = tau[start_idx:end_idx]          # batch_size x 3\n",
    "      s_batch = s[start_idx:end_idx]              # batch_size x 3\n",
    "      s_Ddot_batch = s_Ddot[start_idx:end_idx]    # batch_size x 3\n",
    "\n",
    "      equation = model.Inverse_Dynamic_Eq(t_batch, tau_batch, s_batch, s_Ddot_batch)          # batch_size x 6 x 1\n",
    "\n",
    "      for i in range(equation.shape[0]):\n",
    "        grad_output = torch.zeros_like(equation[:, 3:4])\n",
    "        grad_output[i] = 1\n",
    "\n",
    "        grads = [g.cpu() if g is not None else None for g in torch.autograd.grad(equation[:, 3:4], param_tensors, grad_output, allow_unused=True, create_graph=True)]\n",
    "        grad_row = torch.cat([g.view(-1) if g is not None else torch.zeros(p.numel()) for g, p in zip(grads, param_tensors_cpu)])\n",
    "        jacobian_r1[start_idx + i] = grad_row\n",
    "\n",
    "        grads = [g.cpu() if g is not None else None for g in torch.autograd.grad(equation[:, 4:5], param_tensors, grad_output, allow_unused=True, create_graph=True)]\n",
    "        grad_row = torch.cat([g.view(-1) if g is not None else torch.zeros(p.numel()) for g, p in zip(grads, param_tensors_cpu)])\n",
    "        jacobian_r2[start_idx + i] = grad_row\n",
    "\n",
    "        grads = [g.cpu() if g is not None else None for g in torch.autograd.grad(equation[:, 5:6], param_tensors, grad_output, allow_unused=True, create_graph=True)]\n",
    "        grad_row = torch.cat([g.view(-1) if g is not None else torch.zeros(p.numel()) for g, p in zip(grads, param_tensors_cpu)])\n",
    "        jacobian_r3[start_idx + i] = grad_row\n",
    "\n",
    "\n",
    "    t = X_train['data']['t'].requires_grad_()\n",
    "    tau = X_train['data']['tau']\n",
    "    s = X_train['data']['s']\n",
    "    s_Ddot = X_train['data']['s_Ddot']\n",
    "\n",
    "\n",
    "    batch_size = 32\n",
    "    total_data = t.shape[0]\n",
    "    batches = (total_data + batch_size - 1) // batch_size\n",
    "\n",
    "    jacobian_theta_1 = torch.zeros(total_data, sum(p.numel() for p in param_tensors_cpu))\n",
    "    jacobian_theta_2 = torch.zeros(total_data, sum(p.numel() for p in param_tensors_cpu))\n",
    "    jacobian_theta_3 = torch.zeros(total_data, sum(p.numel() for p in param_tensors_cpu))\n",
    "    jacobian_theta_1_dot = torch.zeros(total_data, sum(p.numel() for p in param_tensors_cpu))\n",
    "    jacobian_theta_2_dot = torch.zeros(total_data, sum(p.numel() for p in param_tensors_cpu))\n",
    "    jacobian_theta_3_dot = torch.zeros(total_data, sum(p.numel() for p in param_tensors_cpu))\n",
    "\n",
    "    for batch in range(batches):\n",
    "      start_idx = batch * batch_size\n",
    "      end_idx = min(start_idx + batch_size, total_data)\n",
    "\n",
    "      t_batch = t[start_idx:end_idx]\n",
    "      tau_batch = tau[start_idx:end_idx]\n",
    "      s_batch = s[start_idx:end_idx]\n",
    "      s_Ddot_batch = s_Ddot[start_idx:end_idx]\n",
    "\n",
    "      u_pred = model.forward(t_batch, tau_batch, s_batch, s_Ddot_batch)         # batch_size x 6\n",
    "\n",
    "      for i in range(len(u_pred)):\n",
    "        grad_output = torch.zeros(u_pred[:, 0].shape)\n",
    "        grad_output[i] = 1\n",
    "\n",
    "        grads = [g.cpu() if g is not None else None for g in torch.autograd.grad(u_pred[:, 0], param_tensors, grad_output, allow_unused=True, create_graph=True)]\n",
    "        grad_row = torch.cat([g.view(-1) if g is not None else torch.zeros(p.numel()) for g, p in zip(grads, param_tensors_cpu)])\n",
    "        jacobian_theta_1[start_idx + i] = grad_row\n",
    "\n",
    "        grads = [g.cpu() if g is not None else None for g in torch.autograd.grad(u_pred[:, 1], param_tensors, grad_output, allow_unused=True, create_graph=True)]\n",
    "        grad_row = torch.cat([g.view(-1) if g is not None else torch.zeros(p.numel()) for g, p in zip(grads, param_tensors_cpu)])\n",
    "        jacobian_theta_2[start_idx + i] = grad_row\n",
    "\n",
    "        grads = [g.cpu() if g is not None else None for g in torch.autograd.grad(u_pred[:, 2], param_tensors, grad_output, allow_unused=True, create_graph=True)]\n",
    "        grad_row = torch.cat([g.view(-1) if g is not None else torch.zeros(p.numel()) for g, p in zip(grads, param_tensors_cpu)])\n",
    "        jacobian_theta_3[start_idx + i] = grad_row\n",
    "\n",
    "        grads = [g.cpu() if g is not None else None for g in torch.autograd.grad(u_pred[:, 3], param_tensors, grad_output, allow_unused=True, create_graph=True)]\n",
    "        grad_row = torch.cat([g.view(-1) if g is not None else torch.zeros(p.numel()) for g, p in zip(grads, param_tensors_cpu)])\n",
    "        jacobian_theta_1_dot[start_idx + i] = grad_row\n",
    "\n",
    "        grads = [g.cpu() if g is not None else None for g in torch.autograd.grad(u_pred[:, 4], param_tensors, grad_output, allow_unused=True, create_graph=True)]\n",
    "        grad_row = torch.cat([g.view(-1) if g is not None else torch.zeros(p.numel()) for g, p in zip(grads, param_tensors_cpu)])\n",
    "        jacobian_theta_2_dot[start_idx + i] = grad_row\n",
    "\n",
    "        grads = [g.cpu() if g is not None else None for g in torch.autograd.grad(u_pred[:, 5], param_tensors, grad_output, allow_unused=True, create_graph=True)]\n",
    "        grad_row = torch.cat([g.view(-1) if g is not None else torch.zeros(p.numel()) for g, p in zip(grads, param_tensors_cpu)])\n",
    "        jacobian_theta_3_dot[start_idx + i] = grad_row\n",
    "\n",
    "      Krr11 = jacobian_r1 @ jacobian_r1.T\n",
    "      Krr22 = jacobian_r2 @ jacobian_r2.T\n",
    "      Krr33 = jacobian_r3 @ jacobian_r3.T\n",
    "      Kuu11 = jacobian_theta_1 @ jacobian_theta_1.T\n",
    "      Kuu22 = jacobian_theta_2 @ jacobian_theta_2.T\n",
    "      Kuu33 = jacobian_theta_3 @ jacobian_theta_3.T\n",
    "      Kuu44 = jacobian_theta_1_dot @ jacobian_theta_1_dot.T\n",
    "      Kuu55 = jacobian_theta_2_dot @ jacobian_theta_2_dot.T\n",
    "      Kuu66 = jacobian_theta_3_dot @ jacobian_theta_3_dot.T\n",
    "\n",
    "      K_trace = (torch.trace(Krr11) + torch.trace(Krr22) + torch.trace(Krr33)) / batch_sizes['PDE'] + \\\n",
    "                (torch.trace(Kuu11) + torch.trace(Kuu22) + torch.trace(Kuu33) + torch.trace(Kuu44) + torch.trace(Kuu55) + torch.trace(Kuu66))/batch_sizes['data']\n",
    "\n",
    "      lambda_1 = K_trace / (torch.trace(Krr11)/batch_sizes['PDE'])\n",
    "      lambda_2 = K_trace / (torch.trace(Krr22)/batch_sizes['PDE'])\n",
    "      lambda_3 = K_trace / (torch.trace(Krr33)/batch_sizes['PDE'])\n",
    "      lambda_4 = K_trace / (torch.trace(Kuu11)/batch_sizes['data'])\n",
    "      lambda_5 = K_trace / (torch.trace(Kuu22)/batch_sizes['data'])\n",
    "      lambda_6 = K_trace / (torch.trace(Kuu33)/batch_sizes['data'])\n",
    "      lambda_7 = K_trace / (torch.trace(Kuu44)/batch_sizes['data'])\n",
    "      lambda_8 = K_trace / (torch.trace(Kuu55)/batch_sizes['data'])\n",
    "      lambda_9 = K_trace / (torch.trace(Kuu66)/batch_sizes['data'])\n",
    "\n",
    "      lambda1 = (lambda_1.item())\n",
    "      lambda2 = (lambda_2.item())\n",
    "      lambda3 = (lambda_3.item())\n",
    "      lambda4 = (lambda_4.item())\n",
    "      lambda5 = (lambda_5.item())\n",
    "      lambda5 = (lambda_5.item())\n",
    "      lambda6 = (lambda_6.item())\n",
    "      lambda7 = (lambda_7.item())\n",
    "      lambda8 = (lambda_8.item())\n",
    "      lambda9 = (lambda_9.item())\n",
    "\n",
    "      return lambda1, lambda2, lambda3, lambda4, lambda5, lambda6, lambda7, lambda8, lambda9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#---------------------------------Getting Data from .mat files-----------------------------#\n",
    "def get_Data(batch_sizes):\n",
    "    x = torch.tensor(h5py.File(\"C:/Users/FPTSHOP/Desktop/Project/simulated_x.mat\", \"r\")[\"data_x\"][3:49998, 1]).unsqueeze(1)                # 1996 x 1\n",
    "    y = torch.tensor(h5py.File(\"C:/Users/FPTSHOP/Desktop/Project/simulated_y.mat\", \"r\")[\"data_y\"][3:49998, 1]).unsqueeze(1)                # 1996 x 1\n",
    "    z = torch.tensor(h5py.File(\"C:/Users/FPTSHOP/Desktop/Project/simulated_z.mat\", \"r\")[\"data_z\"][3:49998, 1]).unsqueeze(1)                # 1996 x 1\n",
    "\n",
    "    s = torch.cat((x, y, z), dim=1) * 0.001                                                                       # 1996 x 3\n",
    "\n",
    "    x_Ddot = torch.tensor(h5py.File(\"C:/Users/FPTSHOP/Desktop/Project/simulated_x_Ddot.mat\", \"r\")[\"data_x_Ddot\"][3:49998, 1]).unsqueeze(1) # 1996 x 1\n",
    "    y_Ddot = torch.tensor(h5py.File(\"C:/Users/FPTSHOP/Desktop/Project/simulated_y_Ddot.mat\", \"r\")[\"data_y_Ddot\"][3:49998, 1]).unsqueeze(1) # 1996 x 1\n",
    "    z_Ddot = torch.tensor(h5py.File(\"C:/Users/FPTSHOP/Desktop/Project/simulated_z_Ddot.mat\", \"r\")[\"data_z_Ddot\"][3:49998, 1]).unsqueeze(1) # 1996 x 1\n",
    "\n",
    "    s_Ddot = torch.cat((x_Ddot, y_Ddot, z_Ddot), dim=1) * 0.001                                                   # 1996 x 3\n",
    "\n",
    "    t = torch.tensor(h5py.File(\"C:/Users/FPTSHOP/Desktop/Project/simulated_x.mat\", \"r\")[\"data_x\"][3:49998, 0]).unsqueeze(1).float()        # 1996 x 1\n",
    "\n",
    "    tau_1 = torch.tensor(scipy.io.loadmat(\"C:/Users/FPTSHOP/Desktop/Project/tau_1.mat\")[\"tau_first_row\"][0, 3:49998]).squeeze().unsqueeze(1)        # 1996 x 1\n",
    "    tau_2 = torch.tensor(scipy.io.loadmat(\"C:/Users/FPTSHOP/Desktop/Project/tau_1.mat\")[\"tau_second_row\"][0, 3:49998]).squeeze().unsqueeze(1)        # 1996 x 1\n",
    "    tau_3 = torch.tensor(scipy.io.loadmat(\"C:/Users/FPTSHOP/Desktop/Project/tau_1.mat\")[\"tau_third_row\"][0, 3:49998]).squeeze().unsqueeze(1)        # 1996 x 1\n",
    "\n",
    "    tau = torch.cat((tau_1, tau_2, tau_3), dim = 1)                                                               # 1996 x 3\n",
    "\n",
    "    theta_1 = torch.tensor(h5py.File(\"C:/Users/FPTSHOP/Desktop/Project/simulated_angle1.mat\", \"r\")[\"angle1\"][3:49998, 1]).unsqueeze(1)     # 1996 x 1\n",
    "    theta_2 = torch.tensor(h5py.File(\"C:/Users/FPTSHOP/Desktop/Project/simulated_angle2.mat\", \"r\")[\"angle2\"][3:49998, 1]).unsqueeze(1)     # 1996 x 1\n",
    "    theta_3 = torch.tensor(h5py.File(\"C:/Users/FPTSHOP/Desktop/Project/simulated_angle3.mat\", \"r\")[\"angle3\"][3:49998, 1]).unsqueeze(1)     # 1996 x 1\n",
    "\n",
    "    theta_1_dot = torch.tensor(h5py.File(\"C:/Users/FPTSHOP/Desktop/Project/simulated_angle1_dot.mat\", \"r\")[\"angle1_dot\"][3:49998, 1]).unsqueeze(1)                # 1996 x 1\n",
    "    theta_2_dot = torch.tensor(h5py.File(\"C:/Users/FPTSHOP/Desktop/Project/simulated_angle2_dot.mat\", \"r\")[\"angle2_dot\"][3:49998, 1]).unsqueeze(1)                # 1996 x 1\n",
    "    theta_3_dot = torch.tensor(h5py.File(\"C:/Users/FPTSHOP/Desktop/Project/simulated_angle3_dot.mat\", \"r\")[\"angle3_dot\"][3:49998, 1]).unsqueeze(1)                # 1996 x 1\n",
    "\n",
    "    u_data = torch.cat((theta_1, theta_2, theta_3, theta_1_dot, theta_2_dot, theta_3_dot), dim = 1)               # 1996 x 6\n",
    "\n",
    "    data = torch.cat((s, s_Ddot, t, tau), dim = 1)                                                                # 1996 x 10\n",
    "\n",
    "    total_points = t.shape[0]                                                                                     # 1996\n",
    "    ids_total = np.arange(total_points)                                                                           # [0, 1, 2, 3, ..., 1994, 1995]\n",
    "\n",
    "    def random_selection(id_set, size):\n",
    "        np.random.seed(seeds_num)\n",
    "        return np.random.choice(id_set, size, replace=False)\n",
    "\n",
    "    def to_tensor(ids):\n",
    "        return {\n",
    "            's'     : data[ids, 0:3].float(),\n",
    "            's_Ddot': data[ids, 3:6].float(),\n",
    "            't'     : data[ids, 6:7].float(),\n",
    "            'tau'   : data[ids, 7:10].float(),\n",
    "            'u'     : u_data[ids, :].float(),\n",
    "        }\n",
    "\n",
    "    id_pde = random_selection(ids_total, batch_sizes['PDE'])\n",
    "    id_data = random_selection(ids_total, batch_sizes['data'])\n",
    "\n",
    "    X_train = {'PDE': to_tensor(id_pde), 'data': to_tensor(id_data)}\n",
    "\n",
    "    all_train_ids = np.union1d(id_pde, id_data)\n",
    "    id_remaining = np.setdiff1d(np.arange(total_points), all_train_ids)\n",
    "\n",
    "    X_test = to_tensor(id_remaining)\n",
    "    X_true = to_tensor(ids_total)\n",
    "\n",
    "    return X_train, X_test, X_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------Implement 2-stage optimizer with the first half of the training utilizes Adam and L-GFGS for the second half----------------------#\n",
    "model = PINN_only_Friction()\n",
    "#dynamic = Dynamic_Model()\n",
    "\n",
    "all_params = list(model.parameters()) + [model.f_c1, model.f_c2, model.f_c3, model.f_v1, model.f_v2, model.f_v3]\n",
    "\n",
    "optimizer_Adam = optim.Adam(all_params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "optimizer_L_BFGS = optim.LBFGS(all_params, lr=0.01, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, history_size=100, line_search_fn=None)\n",
    "\n",
    "\n",
    "def train(model, optimizer_Adam, optimizer_L_BFGS, X_train, changing_point=1.5, iteration=20000):    \n",
    "    \n",
    "    for epoch in range(iteration):\n",
    "\n",
    "        global lambda_1, lambda_2, lambda_3, lambda_4, lambda_5, lambda_6, lambda_7, lambda_8, lambda_9\n",
    "    \n",
    "        loss_r1, loss_r2, loss_r3 = model.pde_loss(X_train['PDE']['t'],\n",
    "                                                   X_train['PDE']['tau'],\n",
    "                                                   X_train['PDE']['s'],\n",
    "                                                   X_train['PDE']['s_Ddot'])\n",
    "        loss_theta_1, loss_theta_2, loss_theta_3, loss_theta_1_dot, loss_theta_2_dot, loss_theta_3_dot = model.data_loss(\n",
    "            X_train['data']['t'],\n",
    "            X_train['data']['tau'],\n",
    "            X_train['data']['s'],\n",
    "            X_train['data']['s_Ddot'],\n",
    "            X_train['data']['u']\n",
    "        )\n",
    "        \n",
    "        \n",
    "        #------------Calculate the total loss--------------#\n",
    "        loss = (lambda_1 * loss_r1 + lambda_2 * loss_r2 + lambda_3 * loss_r3 +\n",
    "                lambda_4 * loss_theta_1 + lambda_5 * loss_theta_2 + lambda_6 * loss_theta_3 +\n",
    "                lambda_7 * loss_theta_1_dot + lambda_8 * loss_theta_2_dot + lambda_9 * loss_theta_3_dot)\n",
    "\n",
    "\n",
    "        \n",
    "        #-----------------------If the current epoch belongs to the first half, use Adam. Otherwise, use L_BGFS-------------------#\n",
    "        if epoch < changing_point * iteration:\n",
    "            optimizer_Adam.zero_grad()      # Zero the gradients\n",
    "            loss.backward()                 # Backpropagate the loss\n",
    "            optimizer_Adam.step()           # Update parameters with Adam\n",
    "        else:\n",
    "            def closure():\n",
    "                loss_r1, loss_r2, loss_r3 = model.pde_loss(X_train['PDE']['t'],\n",
    "                                                           X_train['PDE']['tau'],\n",
    "                                                           X_train['PDE']['s'],\n",
    "                                                           X_train['PDE']['s_Ddot'])\n",
    "                \n",
    "                loss_theta_1, loss_theta_2, loss_theta_3, loss_theta_1_dot, loss_theta_2_dot, loss_theta_3_dot = model.data_loss(\n",
    "                    X_train['data']['t'],\n",
    "                    X_train['data']['tau'],\n",
    "                    X_train['data']['s'],\n",
    "                    X_train['data']['s_Ddot'],\n",
    "                    X_train['data']['u']\n",
    "                )\n",
    "\n",
    "                #------------Calculate the total loss--------------#\n",
    "                loss = (loss_r1 + loss_r2 + loss_r3 +\n",
    "                        loss_theta_1 + loss_theta_2 + loss_theta_3 +\n",
    "                        loss_theta_1_dot + loss_theta_2_dot + loss_theta_3_dot )\n",
    "                \n",
    "                optimizer_L_BFGS.zero_grad()                    # Zero the gradients for L-BFGS\n",
    "                loss.backward(retain_graph=True)                # Backpropagate the loss\n",
    "                return loss\n",
    "            \n",
    "            #-------------------Use L-BFGS for the second half of the iterations---------------------#\n",
    "            optimizer_L_BFGS.step(closure)\n",
    "\n",
    "        model.epoch += 1\n",
    "\n",
    "        \n",
    "        #------------------For each 100 epochs, print out the loss value----------------------------#\n",
    "        if epoch % 100 == 0:\n",
    "                        \n",
    "            print(  f\"Epoch {epoch:04d}, Loss: {loss.item():.6f}, \\n\"\n",
    "                    f\"Loss_residual_1: {loss_r1:.6f}, \"\n",
    "                    f\"Loss_residual_2: {loss_r2:.6f}, \"\n",
    "                    f\"Loss_residual_3: {loss_r3:.6f}, \\n\"\n",
    "                    f\"Loss_theta_1: {loss_theta_1:.6f}, \"\n",
    "                    f\"Loss_theta_2: {loss_theta_2:.6f}, Loss_theta_3: {loss_theta_3:.6f}, \\n\"\n",
    "                    f\"Loss_theta_1_dot: {loss_theta_1_dot:.6f}, \"\n",
    "                    f\"Loss_theta_2_dot: {loss_theta_2_dot:.6f}, \"\n",
    "                    f\"Loss_theta_3_dot: {loss_theta_3_dot:.6f}\"  )\n",
    "            \n",
    "            print(  f\"lambda1 {lambda_1: .6f}, \"\n",
    "                    f\"lambda2 {lambda_2: .6f}, \"\n",
    "                    f\"lambda3 {lambda_3: .6f}, \\n\"\n",
    "                    f\"lambda4 {lambda_4: .6f}, \"\n",
    "                    f\"lambda5 {lambda_5: .6f}, \"\n",
    "                    f\"lambda6 {lambda_6: .6f}, \\n\"\n",
    "                    f\"lambda7 {lambda_7: .6f}, \"\n",
    "                    f\"lambda8 {lambda_8: .6f}, \"\n",
    "                    f\"lambda9 {lambda_9: .6f}, \\n\"   )\n",
    "            \n",
    "            print(  f\"f_v1: {model.f_v1.item(): .6f}, \"\n",
    "                    f\"f_v2: {model.f_v2.item(): .6f}, \"\n",
    "                    f\"f_v3: {model.f_v3.item(): .6f}, \\n\"\n",
    "                    f\"f_c1: {model.f_c1.item(): .6f}, \"\n",
    "                    f\"f_c2: {model.f_c2.item(): .6f}, \"\n",
    "                    f\"f_c3: {model.f_c3.item(): .6f}, \\n\"\n",
    "                    \"|_________________________________| \\n\"   )\n",
    "            \n",
    "            print(\"Gradients of f_c and f_v parameters:\")\n",
    "            print(f\"f_c1 gradient: {model.f_c1.grad}\")\n",
    "            print(f\"f_c2 gradient: {model.f_c2.grad}\")\n",
    "            print(f\"f_c3 gradient: {model.f_c3.grad}\")\n",
    "            print(f\"f_v1 gradient: {model.f_v1.grad}\")\n",
    "            print(f\"f_v2 gradient: {model.f_v2.grad}\")\n",
    "            print(f\"f_v3 gradient: {model.f_v3.grad}\")\n",
    "            \n",
    "\n",
    "            #-----------------------Update adaptive weightings---------------------#\n",
    "            lambda_1, lambda_2, lambda_3, lambda_4, lambda_5, lambda_6, lambda_7, lambda_8, lambda_9 = adaptive_weight(model, X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000, Loss: 36325.863281, \n",
      "Loss_residual_1: 2.357324, Loss_residual_2: 1.963481, Loss_residual_3: 0.473096, \n",
      "Loss_theta_1: 1377.481445, Loss_theta_2: 674.740173, Loss_theta_3: 1039.062622, \n",
      "Loss_theta_1_dot: 165.550659, Loss_theta_2_dot: 169.541412, Loss_theta_3_dot: 149.166168\n",
      "lambda1  8.287048, lambda2  6.476206, lambda3  8.331356, \n",
      "lambda4  11.071783, lambda5  9.669416, lambda6  9.371376, \n",
      "lambda7  10.060722, lambda8  9.860734, lambda9  9.649278, \n",
      "\n",
      "f_v1:  0.851876, f_v2:  0.966759, f_v3:  0.835097, \n",
      "f_c1:  0.925343, f_c2:  0.736948, f_c3:  0.278512, \n",
      "|_________________________________| \n",
      "\n",
      "Gradients of f_c and f_v parameters:\n",
      "f_c1 gradient: 23.267894744873047\n",
      "f_c2 gradient: 14.414267539978027\n",
      "f_c3 gradient: -5.57380485534668\n",
      "f_v1 gradient: 0.9948859810829163\n",
      "f_v2 gradient: 0.25627654790878296\n",
      "f_v3 gradient: -0.09481917321681976\n",
      "Epoch 0100, Loss: 26898.123047, \n",
      "Loss_residual_1: 9.263165, Loss_residual_2: 5.389154, Loss_residual_3: 4.696953, \n",
      "Loss_theta_1: 939.057129, Loss_theta_2: 545.482971, Loss_theta_3: 750.112793, \n",
      "Loss_theta_1_dot: 142.416306, Loss_theta_2_dot: 153.948471, Loss_theta_3_dot: 121.224541\n",
      "lambda1  8.341420, lambda2  6.540304, lambda3  8.259139, \n",
      "lambda4  11.034285, lambda5  9.599100, lambda6  9.369032, \n",
      "lambda7  10.070132, lambda8  9.909471, lambda9  9.579888, \n",
      "\n",
      "f_v1:  0.583071, f_v2:  0.668202, f_v3:  0.549458, \n",
      "f_c1:  0.699230, f_c2:  0.519312, f_c3:  0.040969, \n",
      "|_________________________________| \n",
      "\n",
      "Gradients of f_c and f_v parameters:\n",
      "f_c1 gradient: 47.754981994628906\n",
      "f_c2 gradient: 28.65328598022461\n",
      "f_c3 gradient: 33.205322265625\n",
      "f_v1 gradient: 169.27456665039062\n",
      "f_v2 gradient: 73.44664001464844\n",
      "f_v3 gradient: 140.0364227294922\n",
      "Epoch 0200, Loss: 14545.438477, \n",
      "Loss_residual_1: 3.761539, Loss_residual_2: 2.747079, Loss_residual_3: 1.465800, \n",
      "Loss_theta_1: 868.973389, Loss_theta_2: 556.594543, Loss_theta_3: 655.047791, \n",
      "Loss_theta_1_dot: 92.708572, Loss_theta_2_dot: 74.146286, Loss_theta_3_dot: 91.079872\n",
      "lambda1  29.734406, lambda2  14.525514, lambda3  15.150718, \n",
      "lambda4  3.814599, lambda5  4.798351, lambda6  6.112071, \n",
      "lambda7  24.478207, lambda8  14.337179, lambda9  11.526934, \n",
      "\n",
      "f_v1:  0.268252, f_v2:  0.263629, f_v3:  0.229549, \n",
      "f_c1:  0.412771, f_c2:  0.218564, f_c3: -0.221055, \n",
      "|_________________________________| \n",
      "\n",
      "Gradients of f_c and f_v parameters:\n",
      "f_c1 gradient: 99.69751739501953\n",
      "f_c2 gradient: 43.01141357421875\n",
      "f_c3 gradient: 26.317630767822266\n",
      "f_v1 gradient: 617.42822265625\n",
      "f_v2 gradient: 317.4180908203125\n",
      "f_v3 gradient: 175.88363647460938\n",
      "Epoch 0300, Loss: 11987.347656, \n",
      "Loss_residual_1: 0.673973, Loss_residual_2: 0.390742, Loss_residual_3: 0.610628, \n",
      "Loss_theta_1: 690.543457, Loss_theta_2: 452.197235, Loss_theta_3: 546.944397, \n",
      "Loss_theta_1_dot: 112.319267, Loss_theta_2_dot: 78.163551, Loss_theta_3_dot: 87.904999\n",
      "lambda1  56.260063, lambda2  42.800583, lambda3  85.153091, \n",
      "lambda4  5.934164, lambda5  6.596991, lambda6  4.931475, \n",
      "lambda7  7.364829, lambda8  5.390090, lambda9  9.715107, \n",
      "\n",
      "f_v1:  0.030316, f_v2:  0.081558, f_v3:  0.073123, \n",
      "f_c1:  0.219082, f_c2:  0.056626, f_c3: -0.337885, \n",
      "|_________________________________| \n",
      "\n",
      "Gradients of f_c and f_v parameters:\n",
      "f_c1 gradient: 51.76265335083008\n",
      "f_c2 gradient: 9.770692825317383\n",
      "f_c3 gradient: -0.7318158149719238\n",
      "f_v1 gradient: 257.6954345703125\n",
      "f_v2 gradient: 8.691086769104004\n",
      "f_v3 gradient: 20.228931427001953\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m X_train, X_test, X_true \u001b[38;5;241m=\u001b[39m get_Data(batch_sizes)\n\u001b[0;32m      9\u001b[0m lambda_1, lambda_2, lambda_3, lambda_4, lambda_5, lambda_6, lambda_7, lambda_8, lambda_9 \u001b[38;5;241m=\u001b[39m adaptive_weight(model, X_train)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_Adam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_L_BFGS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 113\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer_Adam, optimizer_L_BFGS, X_train, changing_point, iteration)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf_v3 gradient: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mf_v3\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m#-----------------------Update adaptive weightings---------------------#\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m lambda_1, lambda_2, lambda_3, lambda_4, lambda_5, lambda_6, lambda_7, lambda_8, lambda_9 \u001b[38;5;241m=\u001b[39m \u001b[43madaptive_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 37\u001b[0m, in \u001b[0;36madaptive_weight\u001b[1;34m(model, X_train)\u001b[0m\n\u001b[0;32m     34\u001b[0m grad_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(equation[:, \u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m4\u001b[39m])\n\u001b[0;32m     35\u001b[0m grad_output[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 37\u001b[0m grads \u001b[38;5;241m=\u001b[39m [g\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m]\n\u001b[0;32m     38\u001b[0m grad_row \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([g\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros(p\u001b[38;5;241m.\u001b[39mnumel()) \u001b[38;5;28;01mfor\u001b[39;00m g, p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(grads, param_tensors_cpu)])\n\u001b[0;32m     39\u001b[0m jacobian_r1[start_idx \u001b[38;5;241m+\u001b[39m i] \u001b[38;5;241m=\u001b[39m grad_row\n",
      "File \u001b[1;32mc:\\Users\\FPTSHOP\\anaconda3\\envs\\DemoPublicationEnv\\Lib\\site-packages\\torch\\autograd\\__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[0;32m    493\u001b[0m         grad_outputs_\n\u001b[0;32m    494\u001b[0m     )\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[0;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[0;32m    509\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\FPTSHOP\\anaconda3\\envs\\DemoPublicationEnv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seeds_num = 8386\n",
    "torch.manual_seed(seeds_num)\n",
    "np.random.seed(seeds_num)\n",
    "batch_sizes = {'PDE'  : 100,\n",
    "               'data' : 50}\n",
    "\n",
    "X_train, X_test, X_true = get_Data(batch_sizes)\n",
    "\n",
    "lambda_1, lambda_2, lambda_3, lambda_4, lambda_5, lambda_6, lambda_7, lambda_8, lambda_9 = adaptive_weight(model, X_train)\n",
    "\n",
    "train(model, optimizer_Adam, optimizer_L_BFGS, X_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DemoPublicationEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
